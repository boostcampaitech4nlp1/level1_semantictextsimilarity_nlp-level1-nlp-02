from transformers import AutoTokenizer, AutoModel
from model import ModelBase, ModelFullClasifi

class Selection():
    def __init__(self, model_name, config):
        
        self.config = config
        self.transformer = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = None
        
        self.small = ["klue/roberta-small", "beomi/KcELECTRA-small-v2022", "monologg/koelectra-small-v3-discriminator"]
        self.base = ["klue/roberta-base", "beomi/KcELECTRA-base", "monologg/koelectra-base-v3-discriminator", "beomi/kcbert-base", "jinmang2/kpfbert"]
        self.large = ["klue/roberta-large"]
        
        self.small_first_in = 256
        self.base_first_in = 768
        self.large_first_in = 1024
        
        ## UNK tokenizer add
        if model_name in ["klue/roberta-small", "klue/roberta-base", "klue/roberta-large"]:
            self.add_token_num = self.tokenizer.add_tokens(['ë€', 'í˜“', '##ë¯“', 'ì…§', 'ê´¸', 'íˆ', '##ì–”', '##ã…”', '##ë¬', '##ë¼›', '##í‚µ', 
                                                '##ë„¼', '##ì‘ˆ', 'êº…', '##ì¶‹', '##ê¿', '##ì™˜', '##ë œ', '##ë ', '##ìˆ€', 'ë«', '##ë¡·', 
                                                '##ë©', '##ìš¥', '##ã…‰', '##í', 'ì±—', 'ë¹•', 'êº ', '##ê¿‰', '##ì± ', 'ì¢', 'ì‰‘', '##ì©',
                                                'êº„', 'ë ', '##ã…‘', 'ë¹‚', '##ë´£', '##ì¥´', 'ì•–', 'ì«‘', '##ì¬', 'ì± ', 'ëƒ', '##ëŠ¡', 
                                                '##ëŒ±', '##ì•³', 'ìˆ€', 'ë“„', 'í‘€', 'í‰·', '##ëŠ£', '##í™±', 'ì•œ', '##ì£µ', '##ë‚‘', '##ì¨°',
                                                '##ì•œ', '##ã…', '##ì›Ÿ', 'ëµ›', '##ìŒ°', 'ã…›', '##ë”¨', '##ì¨•', 'ëˆ™', 'ë­', '##íƒ¯', 
                                                'ë–„', 'ìˆ´', '##íš½', '##í', '##ì…©', '##ì¤œ', 'ì›¤', '##ì˜„', 'ã…“', 'ì¢ ', '##á†¢', '##ì ',
                                                'ì½±', '##í›ƒ', 'â˜¼', '##ì›', 'ë…', 'ì´', 'í—¿', '##ì©', '##ë‹ ', '##ë€', '##í™¥', 'ëœ¹', 
                                                'ë²´', 'ìš¯', '##í•³', 'ìµ', 'íš', '##ì°', 'í‚', '##ì§˜', 'ë¯•', '##í—™', '##ëŠ¼', '##ë', 
                                                'ìµ¯', '##ë©±', 'ë—€', '##ìŸŒ', 'ë­', 'ê¼', 'ê»', '##ì ¬', 'ë‚‘', 'ê´', 'í™±', 'ì›©', '##ë„',
                                                '##íœ', 'ì§¦', '##ã…“', 'ê²œ', 'ì—Š', '##ã…›', '##ìŒ', 'ì†“', '##ê²†', 'ë¬', '##ë°ˆ', 'í™“', 
                                                'í­', 'ì›Ÿ', '##ê½ˆ', 'ë§¬', '##ë„´', '##í£', '##í›', 'í½', 'ã…Š', 'ëµ', '##ë€°', '##ìœ±', 
                                                '##ë“•', 'ë', 'ì™˜', 'í€¼', 'ëµŒ', '##ìƒ¸', 'ë½ˆ', 'ë†‹', 'ì¦', '##ì¾ƒ', 'ê¿', '##ì ¼', 'í›…',
                                                '##ë–„', 'ë', 'ìˆ', '##ë‚', 'í™§', '##ì´¤', 'ê°¬', 'ë½œ', '##ì¼¤', 'ì¤¸', '##ì…§', '##ë‡½', 
                                                'ì¯§', 'ã…', 'ì•', '##í™', '##ì·ƒ', 'ë¿¨', 'ë¼›', '##ê´¸', 'ë‡½', 'ì£¤', 'ì¦¤', 'ë¿…', 'ë‚Ÿ', 
                                                'ë´¬', '##ìº…', 'ë™‡', 'ë©', '##ìš¬', '##ì”¸', 'ì°', 'ì¹»', 'ì¬', 'í›', '##ã…', '##ëŸ¤', 
                                                '##ë…', '##ğŸ‘Œ', '##ì¦ ', 'ë•ˆ', '##í”•', 'ì±•', 'í', 'ë¯“', 'ì´', '##ë‚Ÿ', 'í–', 'ëµ€', 
                                                '##ë´¬', '##íŠ­', '##ã„³', 'ã…‰', 'ì˜™', 'í€µ', '##ìŸ', '##ëœ', '##ì±—', '##í™°', 'ë°¨', 'ë´£',
                                                'ì³¬', 'ì˜„', '##ê»€', 'êµ ', 'ë©¥', '##íŠ±', 'ì‘ˆ', 'ì–', 'ìš¥', '##ë–', '##í‘€', '##ì¢', 
                                                'í•³', 'ì•³', '##ì¯§', 'ì†¨', 'í£', 'ì¼¤', 'ì¡‹', '##ì‹€', 'ğŸ‘Œ', '##ì‰‘', '##í‚', 'ëº„', 
                                                '##ì‰˜', '##ì›»', '##ìˆ‘', '##ë€', 'ìš¬', '##ê²œ', '##ë¡¸', 'ë€°', 'ë‹¼', 'ì©', 'ì™¤', 'í› ', 
                                                '##ë„µ', 'ë„µ', 'ê½ˆ', 'ë¯±', 'ê°­', 'ì£µ', '##íƒ†', 'ã…‘', '##ë‹¼', '##êµ£', 'ï½€', '##í©', 
                                                'ì³‡'])
            self.transformer.resize_token_embeddings(self.tokenizer.vocab_size + self.add_token_num)
            
        elif model_name in ["monologg/koelectra-small-v3-discriminator", "monologg/koelectra-base-v3-discriminator"]:
            self.add_token_num = self.tokenizer.add_tokens(['##ë¯“', 'íˆ', '##ì–”', '##ë„¼', '##ì¶‹', '##ì™˜', '##ë œ', '##ë¡·', 'êº ', 'ì¢', 'ë¹‚', 
                                                'ì•–', '##ëŠ¡', '##ëŒ±', 'í‰·', '##ëŠ£', 'ì•œ', '##ì¨°', '##ì•œ', '##ã…', 'ëµ›', '##ìŒ°', 
                                                '##ì¨•', '##íš½', '##ì¤œ', 'ì›¤', '##ì˜„', '##á†¢', 'ì¢ ', '##ì ', '##í›ƒ', 'â˜¼', 'ë…', 
                                                'ì´', '##ë‹ ', '##í™¥', 'ë²´', 'ìš¯', '##í•³', 'ìµ', 'íš', '##ì§˜', 'ë¯•', '##ëŠ¼', '##ë', 
                                                'ìµ¯', 'ë­', 'ê´', '##íœ', 'ì§¦', 'ì†“', '##ê²†', 'í™“', '##í£', '##ë€°', '##ìœ±', '##ë“•', 
                                                'ì™˜', '##ìƒ¸', '##ì¾ƒ', 'ë½œ', 'ì¤¸', 'ì•', '##í™', '##ì·ƒ', 'ë¿¨', 'ì¦¤', '##ìš¬', 'ì¹»', 
                                                '##ëŸ¤', '##ë…', '##ğŸ‘Œ', '##ì¦ ', 'ë•ˆ', 'ë¯“', '##íŠ­', '##ã„³', '##ëœ', 'ë°¨', 'ì³¬', 
                                                'ì˜„', 'êµ ', '##íŠ±', '##ì¢', 'í•³', 'í£', '##ì‹€', 'ğŸ‘Œ', '##ì›»', 'ìš¬', 'ë€°', 'ë‹¼', 
                                                'ë¯±', '##íƒ†', '##ë‹¼', 'ï½€', '##í©'])
            self.transformer.resize_token_embeddings(self.tokenizer.vocab_size + self.add_token_num)
            
        elif model_name in ["beomi/KcELECTRA-base"]:
            self.add_token_num = self.tokenizer.add_tokens(['##ì¾ƒ', 'ë¹‚', '##á†¢', 'ì¹»', 'êµ ', 'ë¿¨', 'ë¯±', '##íƒ†', '##í›ƒ', 'â˜¼', '##ğŸ‘Œ', 'ëµ›', 
                                                            'ğŸ‘Œ', '##ì¦ ', 'ï½€', '##í©'])
            self.transformer.resize_token_embeddings(self.tokenizer.vocab_size + self.add_token_num)
            
        elif model_name in ["beomi/KcELECTRA-small-v2022"]:
            self.add_token_num = self.tokenizer.add_tokens(['ë¯±', '##í™', 'êµ ', 'ë¿¨', '##ì›»', '##íƒ†', 'ëµ€', '##ì¦ ', 'ì¹»', 'ì¢ ', '##ì¨•', 'ë¹‚', 'ëµ›', 
                                                            'ë©¥', 'ìµ¯', '##í›ƒ', '##ëŸ¤', '##í©', 'â˜¼', 'ë²´', 'ë€°', 'ìš¯', '##ë€°', '##ìœ±', '##ì¾ƒ'])
            self.transformer.resize_token_embeddings(self.tokenizer.vocab_size + self.add_token_num)
            
        elif model_name in ["beomi/kcbert-base"]:
            self.add_token_num = self.tokenizer.add_tokens(['ã…', 'ã…“', '##ì¾ƒ', '##á†¢', '>', 'êµ ', 'â€œ', 'ë©¥', 'â€™', '##í›ƒ', 'â˜¼', '##ã…”', 
                                                        'ã…¡', 'ã…£', 'â€¦', '##í™', 'ë¿¨', '##<', '##ã…£', '##á†', '<', 'ìµ¯', '##ì›»', '##ã…‘', 
                                                        'ë¹‚', '##ã…œ', '##ã…¡', 'â€¥', 'ã… ', 'ã…¤', 'ì¹»', '##ã…', '##ì¦ ', '&', 'ë€°', '##ã…“', 
                                                        '##ã…›', 'ã…œ', 'â€', '##ë€°', '##â€¦', 'ë¯±', '##ã… ', 'â€˜', '##ã…', '##íƒ†', 'ã…‘', '##>', 
                                                        'ëµ›', 'ã…›', 'ï½€', '##ã…', '##í©'])
            self.transformer.resize_token_embeddings(self.tokenizer.vocab_size + self.add_token_num)
            
        elif model_name in ["jinmang2/kpfbert"]:
            self.add_token_num = self.tokenizer.add_tokens(['##ë¯“', 'ê´¸', 'íˆ', '##ì–”', '##ì‘ˆ', '##ì¶‹', '##ì™˜', '##ë œ', '##ë ', 'ë«', '##ë¡·', 
                                                        'êº ', 'ì¢', 'ë ', 'ë¹‚', 'ì•–', '##ëŠ¡', '##ëŒ±', 'ë“„', 'í‰·', '##ëŠ£', '##ì¨°', '##ã…', 
                                                        'ëµ›', '##ìŒ°', '##ì¨•', '##íš½', '##ì¤œ', 'ì›¤', '##ì˜„', '##á†¢', 'ì¢ ', '##ì ', '##í›ƒ', 
                                                        'â˜¼', 'ë…', 'ì´', '##ë‹ ', '##í™¥', 'ëœ¹', 'ë²´', 'ìš¯', 'ìµ', 'íš', '##ì§˜', 'ë¯•', '##ëŠ¼',
                                                        '##ë', 'ìµ¯', 'ë­', 'ê¼', 'ê´', '##íœ', 'ì§¦', 'ì†“', '##ê²†', 'í™“', '##í£', '##ë€°', 
                                                        '##ìœ±', '##ë“•', 'ì™˜', '##ìƒ¸', '##ì¾ƒ', 'ë½œ', 'ì¤¸', 'ì•', '##í™', '##ì·ƒ', 'ë¿¨', '##ê´¸',
                                                        'ì¦¤', 'ì¹»', '##ëŸ¤', '##ë…', '##ì¦ ', 'ë•ˆ', 'ë¯“', 'ëµ€', '##íŠ­', '##ã„³', '##ëœ', 'ë°¨',
                                                        'ì³¬', 'ì˜„', 'êµ ', 'ë©¥', '##íŠ±', 'ì‘ˆ', '##ì¢', 'ì†¨', 'í£', '##ì‹€', '##ì›»', '##ë€', 
                                                        '##ë¡¸', 'ë€°', 'ë¯±', '##íƒ†', 'ï½€', '##í©'])
            self.transformer.resize_token_embeddings(self.tokenizer.vocab_size + self.add_token_num)
        
        ## Load Model
        if model_name in self.small:
            if self.config.reg_plus_clasifi_flag or self.config.only_reg_flag or self.config.reg_plus_multi_clasifi_flag:
                self.model = ModelBase.Base(self.transformer, self.small_first_in, self.config.hidden_dropout_prob)
            if self.config.only_clasifi_flag:
                self.model = ModelFullClasifi.FullClasifi(self.transformer, self.small_first_in)
        elif model_name in self.base:
            if self.config.reg_plus_clasifi_flag or self.config.only_reg_flag or self.config.reg_plus_multi_clasifi_flag:
                self.model = ModelBase.Base(self.transformer, self.base_first_in, self.config.hidden_dropout_prob)
            if self.config.only_clasifi_flag:
                self.model = ModelFullClasifi.FullClasifi(self.transformer, self.base_first_in)
        elif model_name in self.large:
            if self.config.reg_plus_clasifi_flag or self.config.only_reg_flag or self.config.reg_plus_multi_clasifi_flag:
                self.model = ModelBase.Base(self.transformer, self.large_first_in, self.config.hidden_dropout_prob)
            if self.config.only_clasifi_flag:
                self.model = ModelFullClasifi.FullClasifi(self.transformer, self.large_first_in)
            
    def get(self):
        return self.model, self.tokenizer